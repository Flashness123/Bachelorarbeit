@online{Donut-Paper,
    author  = {Geewook Kim and Teakgyu Hong and Moonbin Yim and Jeongyeon Nam and Jinyoung Park and Jinyeong Yim and Wonseok Hwang and Sangdoo Yun and Dongyoon Han and Seunghyun Park},
    title   = {OCR-free Document Understanding Transformer},
    year    = {2021},
    url     = {https://arxiv.org/abs/2111.15664},
    note    = {Accessed: 2024-05-22}
}

@article{Attention-Is-All-You-Need,
    author  = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan Gomez and Lukasz Kaiser and Illia Polosukhin},
    title   = {Attention Is All You Need},
    year    = {2017},
    url     = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
    note    = {Accessed: 2024-05-22}
}

@article{Donut-Explained,
    author  = {Ritvik Rastogi},
    title   = {Papers Explained 20: Donut},
    year    = {2023},
    url     = {https://medium.com/dair-ai/papers-explained-20-donut-cb1523bf3281},
    note    = {Accessed: 2024-05-22}
}

@article{SWIN-youtube,
    author = {Soroush Mehraban},
    title = {Swin Transformer - Paper Explained},
    year = {2023},
    url = {https://www.youtube.com/watch?v=qUSPbHE3OeU}
}

@article{SWIN-Transformer-Paper,
    author = {Ze Liu and Yutong Lin and Yue Cao and Han Hu and Yixuan Wei and Zheng Zhang and Stephen Lin and Baining Guo},
    title = {Swin Transformer - Paper Explained},
    year = {2021},
    url = {https://arxiv.org/pdf/2103.14030}
}

@article{BART-Paper,
    author = {Mike Lewis and Yinhan Liu and Naman Goyal and Marjan Ghazvininejad and Abdelrahman Mohamed and Omer Levy and Ves Stoyanov and Luke Zettlemoyer},
    title = {BART-Paper},
    year = {2019},
    url = {https://arxiv.org/pdf/1910.13461v1}
}
@article{layoutlmv3-paper,
    author={Yupan Huang and Tengchao Lv and Lei Cui and Yutong Lu and Furu Wei},
    title={LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking},
    url={https://arxiv.org/pdf/2204.08387},
    year={2022}
}

@article{ViLBERT-paper,
    author={Jiasen Lu and Dhruv Batra and Devi Parikh and Stefan Lee},
    title={ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks},
    year={2019},
    url={https://arxiv.org/pdf/1908.02265}
}

@article{Layoutlmv3-medium,
    author={Shiva Rama},
    title={LayoutLMv3: from zero to hero},
    year={2023},
    url={https://medium.com/@shivarama/layoutlmv3-from-zero-to-hero-part-1-85d05818eec4}
}

@article{Layoutlmv3-easyocr,
    author={Leo Ueno},
    title={Best OCR Models for Text Recognition in Images},
    year={2024},
    url={https://blog.roboflow.com/best-ocr-models-text-recognition/}
}

@article{Fuzzywuzzy,
    author = {Lakshmi Teja Illuri},
    title = {fuzzywuzzy for fuzzy string matching},
    year = {2023},
    url = {https://medium.com/@lakshmiteja.ip/fuzzywuzzy-for-fuzzy-string-matching-25f41c685d6c}
}

@techreport{DONUT-donut-base,
    author = {Geewook Kim and Teakgyu Hong and Moonbin Yim and Jinyoung Park and Jinyeong Yim and Wonseok Hwang and
              Sangdoo Yun and Dongyoon Han and Seunghyun Park},
    title = {Donut: Document Understanding Transformer without {OCR}},
    year = {2021},
    url = {https://huggingface.co/naver-clova-ix/donut-base}
}